{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# [Histogram of Oriented Gradients (HOG) Explained](https://learnopencv.com/histogram-of-oriented-gradients/)\n",
    "\n",
    "## 1. What is a Feature Descriptor?\n",
    "A feature descriptor is a representation of an image (or an image patch) that simplifies the image by extracting useful information while discarding extraneous details. Typically, it converts an image of size _width × height × 3_ (for color channels) into a feature vector of fixed length. For instance, in the case of the HOG descriptor used for pedestrian detection, the input image patch is 64×128×3 and the resulting feature vector has a length of 3780.\n",
    "\n",
    "The key idea is that while the raw pixel values might be useful for viewing an image, they are not optimal for tasks such as image recognition or object detection. Instead, a feature vector produced by descriptors like HOG captures the structural and edge information of an image, which can then be fed into classifiers (e.g., SVM) for high performance in recognition tasks.\n",
    "\n",
    "## 2. What Makes a Good Feature?\n",
    "For classification tasks (such as detecting buttons on shirts or distinguishing them from other circular objects like coins), the features must be:\n",
    "- **Useful:** They capture the essential characteristics of the object (e.g., the edges of a button, its shape, and structure).\n",
    "- **Discriminative:** They are able to differentiate between similar objects (e.g., buttons versus car tires) despite potential similarities in shape.\n",
    "\n",
    "In HOG, the “useful” features are the distribution of the directions of gradients (oriented gradients) within the image. Since gradients highlight areas with abrupt changes in intensity (edges and corners), they effectively encode information about object shape while ignoring uniform regions that contain less useful detail.\n",
    "\n",
    "## 3. How Does the HOG Feature Descriptor Work?\n",
    "The process of computing the HOG feature descriptor involves several key steps:\n",
    "\n",
    "### Step 1: Preprocessing\n",
    "![](https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-f424eed/learnopencv.com/wp-content/uploads/2016/11/hog-preprocessing.jpg)\n",
    "- **Image Patch Selection and Resizing:**\n",
    "  The HOG descriptor for pedestrian detection is computed on a fixed-size patch (typically 64×128). The original image patch (which can have various sizes) is cropped and resized to maintain a fixed aspect ratio (e.g., 1:2) before further processing. This standardization ensures consistency in the descriptor.\n",
    "\n",
    "- **Gamma Correction (Optional):**\n",
    "  Although gamma correction may be applied to normalize the brightness and contrast, its impact is often minor and is sometimes omitted.\n",
    "\n",
    "### Step 2: Calculate Gradient Images\n",
    "![](https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-f424eed/learnopencv.com/wp-content/uploads/2016/11/gradients.png)\n",
    "- **Gradient Computation:**\n",
    "  The image gradients are calculated in both horizontal (x) and vertical (y) directions. This can be done using simple gradient filters or operators like Sobel.\n",
    "  - **Magnitude and Orientation:**\n",
    "    For each pixel, the gradient magnitude is computed to measure the strength of the edge, and the gradient orientation (angle) is computed to determine the edge direction.\n",
    "  These gradients help highlight the edges and contours in the image, which are critical for detecting object shapes.\n",
    "\n",
    "### Step 3: Compute Histograms in 8×8 Cells\n",
    "![](https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-f424eed/learnopencv.com/wp-content/uploads/2016/11/hog-cells.png)\n",
    "- **Dividing the Image:**\n",
    "  The image is divided into small cells, typically 8×8 pixels each.\n",
    "- **Building the Histogram:**\n",
    "  For each cell, a histogram of gradient orientations is computed. This histogram typically has 9 bins covering angles from 0° to 180° (unsigned gradients).\n",
    "  - Each pixel contributes to a bin based on its gradient angle, and the contribution is weighted by its gradient magnitude.\n",
    "  - For example, if a pixel has a gradient angle of 80° and a magnitude of 2, it adds a vote of 2 to the corresponding bin.\n",
    "  - If a pixel's angle falls between two bins, its vote may be split proportionally between them.\n",
    "![](https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-f424eed/learnopencv.com/wp-content/uploads/2016/12/hog-cell-gradients.png)\n",
    "### Step 4: Block Normalization (16×16 Blocks)\n",
    "![](https://learnopencv.com/wp-content/uploads/2016/12/hog-16x16-block-normalization.gif)\n",
    "- **Why Normalize?**\n",
    "  Gradient magnitudes can vary widely due to changes in illumination. Normalizing the histograms makes the descriptor more robust against lighting variations.\n",
    "- **Grouping Cells into Blocks:**\n",
    "  Cells are grouped into larger blocks (typically 2×2 cells, resulting in a 16×16 block).\n",
    "- **Normalization Process:**\n",
    "  The concatenated histogram from the block (which forms a feature vector, for instance, a 36-dimensional vector if there are 4 cells each with a 9-bin histogram) is normalized using techniques like L2 normalization. This process ensures that the descriptor is independent of overall brightness.\n",
    "\n",
    "### Step 5: Construct the Final HOG Feature Vector\n",
    "- **Concatenation:**\n",
    "  The normalized vectors from all the blocks are concatenated to form a single, high-dimensional feature vector.\n",
    "  For the pedestrian detection example, the final feature vector is 3780-dimensional.\n",
    "- **Result:**\n",
    "  This feature vector effectively represents the local shape and structure information in the image patch, making it highly suitable for classification tasks.\n",
    "\n",
    "## 4. Summary\n",
    "- **HOG as a Feature Descriptor:**\n",
    "  HOG extracts the distribution of gradient orientations from an image patch, capturing the essential edge and shape information while discarding less useful data.\n",
    "- **Key Steps Involved:**\n",
    "  - **Preprocessing:** Resize the image patch to a fixed size while maintaining a uniform aspect ratio.\n",
    "  - **Gradient Computation:** Calculate the magnitude and direction of gradients to highlight edges.\n",
    "  - **Histogram Calculation:** Divide the image into 8×8 cells and compute a 9-bin histogram of gradients for each cell.\n",
    "  - **Block Normalization:** Group cells into 16×16 blocks, concatenate their histograms, and normalize the block vectors to mitigate the effects of varying illumination.\n",
    "  - **Feature Vector Construction:** Concatenate all normalized block vectors into a single feature vector (e.g., 3780 dimensions for a 64×128 patch).\n",
    "- **Utility:**\n",
    "  The resulting HOG feature vector is robust and discriminative, making it effective for tasks like object detection (e.g., pedestrian detection) and image recognition when used with classifiers such as SVM.\n",
    "\n",
    "This detailed process shows how HOG transforms an image patch into a compact and powerful feature descriptor that is well-suited for various computer vision applications.\n",
    "\n"
   ],
   "id": "fe58084fbec20dbf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-11T03:31:46.240704Z",
     "start_time": "2025-03-11T03:31:46.236975Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression # Handle overlapping"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:05:55.878843Z",
     "start_time": "2025-03-11T04:05:50.584353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(r\"C:\\Users\\alaas\\OneDrive\\Desktop\\practical\\Notebooks\\Computer Vision\\Next-Gen Computer Vision\\images\\walking.avi\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame\")\n",
    "        break\n",
    "\n",
    "    hog = cv2.HOGDescriptor()\n",
    "\n",
    "    # Loads a pre-trained SVM detector designed for people detection.\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    (bounding_boxes,weights)=hog.detectMultiScale(frame,winStride=(8,8),padding=(4,4),scale=1.01)\n",
    "\n",
    "    # Get rid of overlapping bounding boxes You can tweak the overlapThresh value for better results\n",
    "    # convert the bounding box format from (x, y, width, height) to (x1, y1, x2, y2), which is required by Non-Maximum Suppression (NMS).\n",
    "    bounding_boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in bounding_boxes])\n",
    "    selection = non_max_suppression(bounding_boxes,  probs=None, overlapThresh=0.8)\n",
    "\n",
    "    # draw the final bounding boxes\n",
    "    for (x1, y1, x2, y2) in selection:\n",
    "        cv2.rectangle(frame,\n",
    "                     (x1, y1),\n",
    "                     (x2, y2),\n",
    "                     (0, 255, 0),\n",
    "                      4)\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break  # Exit loop when 'q' is pressed\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "739ad48482b55347",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "\n",
    "## HOG.detectMultiScale Parameters\n",
    "\n",
    "### winStride\n",
    "- **What it does:**\n",
    "  Sets the step size (in pixels) for moving the fixed-size sliding window over the image.\n",
    "- **Effect on Detection:**\n",
    "  - **Accuracy:**\n",
    "    Smaller strides (e.g., (4,4) or (8,8)) mean more overlapping windows, which increases the chance of capturing pedestrians (especially smaller or partially visible ones).\n",
    "  - **Speed:**\n",
    "    A larger stride (e.g., (16,16)) evaluates fewer windows, speeding up detection but risking missed detections.\n",
    "\n",
    "### padding\n",
    "- **What it does:**\n",
    "  Adds extra pixels (in both x and y directions) around the sliding window.\n",
    "- **Effect on Detection:**\n",
    "  - **Accuracy:**\n",
    "    Extra padding helps capture additional contextual information; for example, if a pedestrian is near the edge of a window, the padded area can help include crucial body parts.\n",
    "  - **Caveat:**\n",
    "    Too much padding might bring in irrelevant background information that can confuse the classifier.\n",
    "\n",
    "### scale\n",
    "- **What it does:**\n",
    "  Specifies the factor by which the image is resized at each layer of the image pyramid.\n",
    "- **Effect on Detection:**\n",
    "  - **Accuracy:**\n",
    "    A small scale factor (close to 1.0, e.g., 1.01) creates many pyramid levels so that even slight differences in object size are captured, which improves sensitivity.\n",
    "  - **Speed:**\n",
    "    More pyramid levels increase the computational load. A larger scale factor (e.g., 1.05 or 1.1) reduces the number of scales—and thus speeds up detection—but may miss objects that vary subtly in size.\n",
    "\n",
    "---\n",
    "\n",
    "## Non-Maximum Suppression (NMS) Parameters\n",
    "\n",
    "After running HOG.detectMultiScale, you typically obtain multiple overlapping bounding boxes. NMS is used to retain the most confident ones.\n",
    "\n",
    "### bounding_boxes\n",
    "- **What it does:**\n",
    "  An array (or list) of bounding boxes in the format (x1, y1, x2, y2) that represents the detected regions.\n",
    "- **Note:**\n",
    "  Often, you need to convert from (x, y, width, height) to (x1, y1, x2, y2) before applying NMS.\n",
    "\n",
    "### probs (or weights)\n",
    "- **What it does:**\n",
    "  A list of confidence scores for each bounding box detection.\n",
    "- **Usage:**\n",
    "  When provided, NMS uses these scores to decide which overlapping box to keep.\n",
    "- **In our example:**\n",
    "  If you don’t have separate scores, you can pass `None` so that NMS treats all boxes equally.\n",
    "\n",
    "### overlapThresh\n",
    "- **What it does:**\n",
    "  Defines the threshold for how much two bounding boxes can overlap before one is suppressed.\n",
    "- **Effect on Detection:**\n",
    "  - **Accuracy:**\n",
    "    A lower threshold (e.g., 0.3–0.45) means boxes that overlap more than this percentage are merged or suppressed, reducing false positives from multiple detections of the same person.\n",
    "  - **Tuning:**\n",
    "    Adjusting this value helps balance between removing redundant boxes and accidentally suppressing true, nearby detections.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| **Parameter**       | **Function**                                                  | **Detection Impact**                                                       |\n",
    "|---------------------|---------------------------------------------------------------|----------------------------------------------------------------------------|\n",
    "| **winStride**       | Step size for sliding window movement                         | Smaller → More windows (higher accuracy, slower speed); Larger → Fewer windows (faster, may miss objects)  |\n",
    "| **padding**         | Extra pixels added around each window                         | Helps include context; too high may add noise                             |\n",
    "| **scale**           | Factor for image resizing in the pyramid                      | Smaller (close to 1) → More levels (improved sensitivity, slower); Larger → Fewer levels (faster, less sensitive) |\n",
    "| **bounding_boxes**  | List of detected box coordinates (converted to (x1,y1,x2,y2))   | Required input for NMS                                                    |\n",
    "| **probs (weights)** | Confidence scores for each detection                           | Used by NMS to select the most confident detection                         |\n",
    "| **overlapThresh**   | Maximum allowed overlap for two boxes before suppression       | Lower value → Aggressive suppression; higher value → more boxes kept        |\n"
   ],
   "id": "d745d71ed153a89d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
